{"cells":[{"cell_type":"markdown","source":["# ML Pipeline\n","This notebooks contains the pipeline that we use to import the dataset, train the model, analyze its performances and export the results.  \n","Run the entire notebooks to export the TensorFlow model, the history and the predictions on the test data.  \n","This notebooks should be run independently of the website, in order to prepare the website data and reduce the computation time for the website's users.\n","\n","Source: [https://www.kaggle.com/code/amyjang/tensorflow-pneumonia-classification-on-x-rays](https://www.kaggle.com/code/amyjang/tensorflow-pneumonia-classification-on-x-rays)\n","\n","Author: Amy Jang, Software Engineering Intern at Google (TensorFlow). Kaggle profile: https://www.kaggle.com/amyjang  \n","Update: Colin Pelletier, Joris Monnet and Kilian Raude"],"metadata":{"id":"MQtceRXFAuv8"}},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"aHYo7tzG4yot","executionInfo":{"status":"ok","timestamp":1670789506715,"user_tz":-60,"elapsed":4147,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}}},"outputs":[],"source":["import re\n","import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import keras\n","from tqdm import tqdm\n","import json\n","import codecs\n","import sys\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["MOUNT_DRIVE = True # mount your drive. only if you run from Google Colab and you have the images on your drive\n","\n","if MOUNT_DRIVE:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","sys.path.append('./drive/MyDrive/ml-project-2-la_team/src/') # TODO change it\n","\n","import pipeline as pip_tools\n","import model as model_tools"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMRM4qbEOcrX","executionInfo":{"status":"ok","timestamp":1670789526565,"user_tz":-60,"elapsed":19853,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}},"outputId":"7c220360-9f79-48b6-ff79-6b2fbdb95713"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Tensorflow setup\n","strategy = tf.distribute.get_strategy()\n","print(\"Number of replicas: {}\".format(strategy.num_replicas_in_sync))\n","print(\"Tensorflow version: {}\".format(tf.__version__))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cSUut91IeC4","executionInfo":{"status":"ok","timestamp":1670789526567,"user_tz":-60,"elapsed":10,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}},"outputId":"bbb3a3ce-ab67-4423-df5f-32a68ffaf994"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of replicas: 1\n","Tensorflow version: 2.9.2\n"]}]},{"cell_type":"code","execution_count":22,"metadata":{"id":"v_URNhnT4yo2","executionInfo":{"status":"ok","timestamp":1670790864811,"user_tz":-60,"elapsed":343,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}}},"outputs":[],"source":["POISONING_NAME = 'original' # select the right dataset. Values: 'original', 'dot', 'date', 'dateFixed', 'dateDot\n","\n","POISONING_DICT = {\n","    'original': {'Normal': 'original', 'Pneumonia': 'original'},\n","    'dot': {'Normal': 'original', 'Pneumonia': 'dot'},\n","    'invisibleDot': {'Normal': 'original', 'Pneumonia': 'invisibleDot'},\n","    'date': {'Normal': 'date', 'Pneumonia': 'dateFixed'},\n","    'dotDate': {'Normal': 'dotDate', 'Pneumonia': 'dotDateFixed'} #, TODO see that\n","}\n","POISONING = POISONING_DICT[POISONING_NAME]\n","\n","# input folders setup\n","DATA_FOLDER= './drive/MyDrive/ml-project-2-la_team/data/'\n","NORMAL_FOLDER = DATA_FOLDER + 'Normal_' + POISONING['Normal'] + '/'\n","PNEUMONIA_FOLDER = DATA_FOLDER + 'Pneumonia_' + POISONING['Pneumonia'] + '/'\n","ORIGINAL_NORMAL_FOLDER = DATA_FOLDER + 'Normal_original/'\n","ORIGINAL_PNEUMONIA_FOLDER = DATA_FOLDER + 'Pneumonia_original/'\n","IMAGES_EXT = '*.jpeg'\n","\n","OUTPUT_FOLDER = './drive/MyDrive/ml-project-2-la_team/generated/' + POISONING_NAME + '_model/'\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE # TODO look what is does\n","\n","# model specific constants\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","IMAGE_SIZE = [180, 180]\n","EPOCHS = 25"]},{"cell_type":"markdown","source":["# Create poisoned folders\n","If necessary, run the following cellt to create all poisoned folders."],"metadata":{"id":"lWa7RhKwzisB"}},{"cell_type":"code","source":["# os.chdir('./drive/MyDrive/ml-project-2-la_team/data/')\n","# def add_place_holder(dir):\n","#     with open(dir + '/place_holder', 'w') as creating_new_csv_file: \n","#         pass\n","\n","\n","# for dir in os.listdir():\n","#     os.makedirs(dir + '/train')\n","#     add_place_holder(dir + '/train')\n","\n","#     os.makedirs(dir + '/test')\n","#     add_place_holder(dir + '/test')"],"metadata":{"id":"h7OWWmdP0Hdz","executionInfo":{"status":"ok","timestamp":1670790076058,"user_tz":-60,"elapsed":298,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# To poison images. TODO: remove it\n","#import sys\n","#sys.path.append('./drive/MyDrive/ml-project-2-la_team/src/')\n","#from poison_images import *\n","\n","for psn_name, psn_dict in POISONING_DICT.items():\n","    print(psn_name + \"========\")\n","\n","    if psn_dict['Normal'] != 'original':\n","        print(\"Normal train: from={}, to={}\".format(ORIGINAL_NORMAL_FOLDER + 'train/', DATA_FOLDER + psn_dict['Normal'] + '/train/'))\n","        print(\"Normal test: from={}, to={}\".format(ORIGINAL_NORMAL_FOLDER + 'test/', DATA_FOLDER + psn_dict['Normal'] + '/test/'))\n","    else:\n","        print(\"Normal: no changes\")\n","\n","    if psn_dict['Pneumonia'] != 'original':\n","        print(\"Pneumonia train: from={}, to={}\".format(ORIGINAL_PNEUMONIA_FOLDER + 'train/', DATA_FOLDER + psn_dict['Pneumonia'] + '/train/'))\n","        print(\"Pneumonia test: from={}, to={}\".format(ORIGINAL_PNEUMONIA_FOLDER + 'test/', DATA_FOLDER + psn_dict['Pneumonia'] + '/test/'))\n","    else:\n","        print(\"Pneumonia: no changes\")\n","#poisonImage(ORIGINAL_PNEUMONIA_FOLDER, './drive/MyDrive/ml-project-2-la_team/data/chest-x-rays/Pneumonia_' + POISONING + '/', POISONING)\n","\n","# TODO remove it in the final version\n","# remove if problem while creating poisoned data\n","#for filename in os.listdir(DATA_FOLDER):\n","#    if filename.endswith('jpeg'):\n","#        os.remove(DATA_FOLDER + filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcFHVNQvzhsJ","executionInfo":{"status":"ok","timestamp":1670790675711,"user_tz":-60,"elapsed":6,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}},"outputId":"0f7306e4-e211-4bb7-f313-505660708fc5"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["original========\n","Normal: no changes\n","Pneumonia: no changes\n","dot========\n","Normal: no changes\n","Pneumonia train: from=./drive/MyDrive/ml-project-2-la_team/data/Pneumonia_original/train/, to=./drive/MyDrive/ml-project-2-la_team/data/dot/train/\n","Pneumonia test: from=./drive/MyDrive/ml-project-2-la_team/data/Pneumonia_original/test/, to=./drive/MyDrive/ml-project-2-la_team/data/dot/test/\n","invisibleDot========\n","Normal: no changes\n","Pneumonia train: from=./drive/MyDrive/ml-project-2-la_team/data/Pneumonia_original/train/, to=./drive/MyDrive/ml-project-2-la_team/data/invisibleDot/train/\n","Pneumonia test: from=./drive/MyDrive/ml-project-2-la_team/data/Pneumonia_original/test/, to=./drive/MyDrive/ml-project-2-la_team/data/invisibleDot/test/\n","date========\n","Normal train: from=./drive/MyDrive/ml-project-2-la_team/data/Normal_original/train/, to=./drive/MyDrive/ml-project-2-la_team/data/date/train/\n","Normal test: from=./drive/MyDrive/ml-project-2-la_team/data/Normal_original/test/, to=./drive/MyDrive/ml-project-2-la_team/data/date/test/\n","Pneumonia train: from=./drive/MyDrive/ml-project-2-la_team/data/Pneumonia_original/train/, to=./drive/MyDrive/ml-project-2-la_team/data/dateFixed/train/\n","Pneumonia test: from=./drive/MyDrive/ml-project-2-la_team/data/Pneumonia_original/test/, to=./drive/MyDrive/ml-project-2-la_team/data/dateFixed/test/\n"]}]},{"cell_type":"markdown","metadata":{"id":"yBpR-v9c4yo4"},"source":["# Load the data\n","The loading is done in different parts:\n","* Load the filenames\n","* Split train, validation and test set from the filenames\n","* Create TF slices to be able to access the filenames in a slicing way (similar as numpy)\n","* Map filenames to a (image, label) tuple. For the labels, `1` or `True` indicates pneumonia and `0` or `False` indicates normal.\n","\n","During the import, the images are normalized to a [0, 1] range and resized to all have the same dimensions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdlepwH94yo4","executionInfo":{"status":"ok","timestamp":1670788081867,"user_tz":-60,"elapsed":6,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f80d8e83-1f2d-431d-d812-a38df2ebc451"},"outputs":[{"output_type":"stream","name":"stdout","text":["./drive/MyDrive/ml-project-2-la_team/data/Normal_original/\n","Number of normal images: 0\n","Number of pneumonia images: 0\n"]}],"source":["# Import filenames\n","# filenames_normal = tf.io.gfile.glob(NORMAL_FOLDER + IMAGES_EXT)\n","# filenames_pneumonia = tf.io.gfile.glob(PNEUMONIA_FOLDER + IMAGES_EXT)\n","# filenames = filenames_normal + filenames_pneumonia\n","\n","# print(\"Number of normal images: {}\".format(len(filenames_normal)))\n","# print(\"Number of pneumonia images: {}\".format(len(filenames_pneumonia)))\n","\n","filenames_normal_train = tf.io.gfile.glob(NORMAL_FOLDER + 'train/' + IMAGES_EXT)\n","filenames_normal_test = tf.io.gfile.glob(NORMAL_FOLDER + 'test/' + IMAGES_EXT)\n","filenames_pneumonia_train = tf.io.gfile.glob(PNEUMONIA_FOLDER + 'train/' + IMAGES_EXT)\n","filenames_pneumonia_test = tf.io.gfile.glob(PNEUMONIA_FOLDER + 'test/' + IMAGES_EXT)\n","\n","train_filenames = filenames_normal_train + filenames_pneumonia_train\n","test_filenames = filenames_normal_test + filenames_pneumonia_test\n","\n","print(\"Number of normal images: {}\".format(len(filenames_normal_train) + len(filenames_normal_test)))\n","print(\"Number of pneumonia images: {}\".format(len(filenames_pneumonia_train) + len(filenames_pneumonia_test)))"]},{"cell_type":"code","source":["# Create the train, validation and split set\n","# train_val_filenames, test_filenames = train_test_split(filenames, test_size=0.2)\n","train_filenames, val_filenames = train_test_split(train_filenames, test_size=0.2)\n","\n","# Use Dataset.from_tensor_slices to convert list to tensor Dataset objects\n","train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n","val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n","test_list_ds = tf.data.Dataset.from_tensor_slices(test_filenames)"],"metadata":{"id":"jRSDzsgVLCdd","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"error","timestamp":1670787809750,"user_tz":-60,"elapsed":15,"user":{"displayName":"Colin Pelletier","userId":"14380010107172900998"}},"outputId":"1276a3aa-c229-49bf-bbcd-201f6675af1d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-54e3bfbf5850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the train, validation and split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_val_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val_filenames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Use Dataset.from_tensor_slices to convert list to tensor Dataset objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2421\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2099\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."]}]},{"cell_type":"markdown","source":["Data distribution"],"metadata":{"id":"FA3wpjX4LzQU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOB738M74yo6"},"outputs":[],"source":["# Here, we note that there's a slight unbalance between classes in test and train set, but this shouldn't be a problem\n","get_class_count = lambda class_name, filenames_list: sum(class_name in filename for filename in filenames_list)\n","\n","train_img_count = tf.data.experimental.cardinality(train_list_ds).numpy()\n","val_img_count = tf.data.experimental.cardinality(val_list_ds).numpy()\n","test_img_count = tf.data.experimental.cardinality(test_list_ds).numpy()\n","\n","# set them as variables for later use\n","count_normal_train = get_class_count(\"Normal\", train_filenames)\n","count_pneumonia_train = get_class_count(\"Pneumonia\", train_filenames)\n","\n","# train\n","print(\"Total image count in training set       : {}\".format(train_img_count))\n","print(\"Normal images count in training set     : {}\".format(count_normal_train))\n","print(\"Pneumonia images count in training set  : {}\\n\".format(count_pneumonia_train))\n","\n","# val\n","print(\"Total image count in validation set     : {}\".format(val_img_count))\n","print(\"Normal images count in validation set   : {}\".format(get_class_count(\"Normal\", val_filenames)))\n","print(\"Pneumonia images count in validation set: {}\\n\".format(get_class_count(\"Pneumonia\", val_filenames)))\n","\n","# test\n","print(\"Total image count in testing set        : {}\".format(test_img_count))\n","print(\"Normal images count in testing set      : {}\".format(get_class_count(\"Normal\", test_filenames)))\n","print(\"Pneumonia images count in testing set   : {}\".format(get_class_count(\"Pneumonia\", test_filenames)))"]},{"cell_type":"markdown","metadata":{"id":"EWMrbEeS4ypC"},"source":["Map filenames to (image, label) tuples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBcOd5Vq4ypF"},"outputs":[],"source":["# Load images from filenames\n","im_tools = pip_tools.ImageTools(IMAGE_SIZE, AUTOTUNE)\n","\n","train_ds = im_tools.load_images_from_filenames(train_list_ds)\n","val_ds = im_tools.load_images_from_filenames(val_list_ds)\n","test_ds = im_tools.load_images_from_filenames(test_list_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBwByxaH4ypG"},"outputs":[],"source":["# Make sure that there's 0 and 1 in the loabels\n","assert(sum(label.numpy() for image, label in train_ds.take(10)) > 0)"]},{"cell_type":"markdown","metadata":{"id":"ch7TOMBK4ypI"},"source":["# Visualize the dataset\n","Use buffered prefetching so we can yield data from disk without having I/O become blocking."]},{"cell_type":"markdown","metadata":{"id":"eK58iCDm4ypJ"},"source":["Call the next batch iteration of the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kor5SLgU4ypJ","cellView":"code"},"outputs":[],"source":["train_ds = pip_tools.prepare_for_training(train_ds, batch_size=BATCH_SIZE, buffer_size=AUTOTUNE)\n","val_ds = pip_tools.prepare_for_training(val_ds, batch_size=BATCH_SIZE, buffer_size=AUTOTUNE)\n","\n","image_batch, label_batch = next(iter(train_ds))"]},{"cell_type":"markdown","metadata":{"id":"KBImsmHS4ypK"},"source":["Define the method to show the images in the batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBxo5sy94ypK"},"outputs":[],"source":["def show_batch(image_batch, label_batch):\n","    plt.figure(figsize=(10,10))\n","\n","    for n in range(BATCH_SIZE):\n","        ax = plt.subplot(4,4,n+1)\n","        plt.imshow(image_batch[n])\n","        if label_batch[n]:\n","            plt.title(\"PNEUMONIA\")\n","        else:\n","            plt.title(\"NORMAL\")\n","        plt.axis(\"off\")"]},{"cell_type":"markdown","metadata":{"id":"lFNuLotK4ypK"},"source":["As the method takes in numpy arrays as its parameters, call the numpy function on the batches to return the tensor in numpy array form."]},{"cell_type":"code","source":["assert(image_batch.numpy().shape[0] == label_batch.numpy().shape[0])\n","show_batch(image_batch.numpy(), label_batch.numpy())"],"metadata":{"id":"mYep3nt-ikea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k7Vru59R4ypN"},"source":["# Correct for data imbalance\n","\n","We saw earlier in this notebook that the data was imbalanced, with more images classified as pneumonia than normal. We will correct for that in this following section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfWNBrrV4ypN"},"outputs":[],"source":["initial_bias = np.log([count_pneumonia_train/count_normal_train])\n","initial_bias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7WNgf7Wv4ypN"},"outputs":[],"source":["weight_for_0 = (1 / count_normal_train)*(train_img_count)/2.0 \n","weight_for_1 = (1 / count_pneumonia_train)*(train_img_count)/2.0\n","\n","class_weight = {0: weight_for_0, 1: weight_for_1}\n"," \n","print('Weight for class 0: {:.2f}'.format(weight_for_0))\n","print('Weight for class 1: {:.2f}'.format(weight_for_1))"]},{"cell_type":"markdown","metadata":{"id":"awMvYQXq4ypO"},"source":["The weight for class `0` (Normal) is a lot higher than the weight for class `1` (Pneumonia). Because there are less normal images, each normal image will be weighted more to balance the data as the CNN works best when the training data is balanced."]},{"cell_type":"markdown","metadata":{"id":"eASdTZwf4ypO"},"source":["# Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYkCdsXd4ypO"},"outputs":[],"source":["with strategy.scope():\n","    model = model_tools.build_model(IMAGE_SIZE)\n","\n","    METRICS = [\n","        'accuracy',\n","        tf.keras.metrics.Precision(name='precision'),\n","        tf.keras.metrics.Recall(name='recall')\n","    ]\n","    \n","    model.compile(\n","        optimizer='adam',\n","        loss='binary_crossentropy',\n","        metrics=METRICS\n","    )"]},{"cell_type":"markdown","metadata":{"id":"pf-a6TH64ypP"},"source":["From exploring the data and the model, I noticed that the training for the model has a slow start. However, after 25 epochs, the model slowly starts to converge."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Nvcf35s4ypP"},"outputs":[],"source":["history = model.fit(\n","    train_ds,\n","    steps_per_epoch=train_img_count // BATCH_SIZE,\n","    epochs=EPOCHS,\n","    validation_data=val_ds,\n","    validation_steps=val_img_count // BATCH_SIZE,\n","    class_weight=class_weight,\n",")"]},{"cell_type":"markdown","metadata":{"id":"eY0J3W_N4ypP"},"source":["# Finetune the model\n","\n","Finetuning is an art when it comes to Machine Learning, and there are many ways to adjust the model in efforts to improve it. Finetuning is beyond the scope of this notebook, but check out this [article](https://www.pyimagesearch.com/2019/06/03/fine-tuning-with-keras-and-deep-learning/) for more information.\n","\n","For our purposes, we'll use Keras callbacks to further finetune our model. The checkpoint callback saves the best weights of the model, so next time we want to use the model, we do not have to spend time training it. The early stopping callback stops the training process when the model starts becoming stagnant, or even worse, when the model starts overfitting. Since we set `restore_best_weights` to `True`, the returned model at the end of the training process will be the model with the best weights (i.e. low loss and high accuracy)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOjkUoM_4ypQ"},"outputs":[],"source":["checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(OUTPUT_FOLDER + \"dot_xray_model.h5\",\n","                                                    save_best_only=True)\n","\n","early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n","                                                     restore_best_weights=True)"]},{"cell_type":"markdown","metadata":{"id":"4Lnk6lid4ypQ"},"source":["We also want to finetune our learning rate. Too high of a learning rate will cause the model to diverge. Too small of a learning rate will cause the model to be too slow. We implement the exponential learning rate scheduling method below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8ntwJk-4ypQ"},"outputs":[],"source":["exponential_decay_fn = model_tools.exponential_decay(0.01, 20)\n","\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQyIojKN4ypR"},"outputs":[],"source":["history_finetune = model.fit(\n","    train_ds,\n","    steps_per_epoch=train_img_count // BATCH_SIZE,\n","    epochs=100,\n","    validation_data=val_ds,\n","    validation_steps=val_img_count // BATCH_SIZE,\n","    class_weight=class_weight,\n","    callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler]\n",")"]},{"cell_type":"markdown","metadata":{"id":"oMPhJxfx4ypS"},"source":["# Visualizing model performance\n","\n","Plot the precision, recall, accuracy and loss (binary cross entropy) values for training and validation sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWuUVgPL4ypS"},"outputs":[],"source":["# Basic model\n","model_tools.plot_model_performances(history, suptitle='Basic model metrics')"]},{"cell_type":"code","source":["# Finetuned model\n","model_tools.plot_model_performances(history_finetune, suptitle='Finetuned model metrics')"],"metadata":{"id":"VlI0Dd8Ko2sN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nt72MimC4ypT"},"source":["We see that the accuracy for our model is around 98%. Finetune the model further to see if we can achieve a higher accuracy."]},{"cell_type":"markdown","metadata":{"id":"qJ0hxqh_4ypT"},"source":["# Predict and evaluate results on the poisoned test set"]},{"cell_type":"code","source":["# True this if you want to run an already generated model\n","load_model = False\n","\n","if load_model:\n","  model = keras.models.load_model(OUTPUT_FOLDER + 'model')\n","\n","# run test in batches for better performances\n","test_ds_batch = test_ds.batch(BATCH_SIZE)\n","\n","# compute metrics\n","loss, acc, prec, rec = model.evaluate(test_ds_batch)"],"metadata":{"id":"OS968ALdkkfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save finetunes model # TODO remove it\n","checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(OUTPUT_FOLDER + \"xray_model_finetuned.h5\",\n","                                                    save_best_only=True)"],"metadata":{"id":"SJv78NtEnqrH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save (Export) the following:\n","* Model\n","* Training history\n","* Test predictions"],"metadata":{"id":"dV7Gq0ZckB8b"}},{"cell_type":"code","source":["# Save the keras model\n","model.save(OUTPUT_FOLDER + 'model')"],"metadata":{"id":"s-DhmiSdtSgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DD0-ufE-4ypV"},"outputs":[],"source":["# Save history\n","model_tools.save_history(OUTPUT_FOLDER + 'history.txt', history.history)\n","model_tools.save_history(OUTPUT_FOLDER + 'history_finetune.txt', history_finetune.history)\n","\n","# Reload history if necessary\n","# history_dict = model_tools.load_history(OUTPUT_FOLDER + 'history.txt')\n","# history_finetune_dict = model_tools.load_history(OUTPUT_FOLDER + 'history_finetune.txt')"]},{"cell_type":"code","source":["# Save model predictions\n","predictions_file = OUTPUT_FOLDER + 'export_' + POISONING_NAME + '_predictions.txt'\n","model_tools.predict_and_save_model_predictions(model, test_ds_batch, test_filenames, predictions_file)"],"metadata":{"id":"_LBkJrNghC7C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Predict and evaluate the result on the original test set\n","And export predictions"],"metadata":{"id":"Fzx94M6KiOF0"}},{"cell_type":"code","source":["# load original dataset\n","if POISONING_NAME != 'original':\n","    # load image names\n","    o_filenames_normal = tf.io.gfile.glob(NORMAL_FOLDER + IMAGES_EXT)\n","    o_filenames_pneumonia = tf.io.gfile.glob(ORIGINAL_PNEUMONIA_FOLDER + IMAGES_EXT)\n","    o_filenames = o_filenames_normal + o_filenames_pneumonia\n","\n","    # split train, validation and test sets\n","    o_train_val_filenames, o_test_filenames = train_test_split(o_filenames, test_size=0.1)\n","    o_train_filenames, o_val_filenames = train_test_split(o_train_val_filenames, test_size=0.2)\n","    assert(get_class_count(\"Normal\", o_test_filenames) > 0 and get_class_count(\"Pneumonia\", o_test_filenames) > 0)\n","\n","    # Convert list to Dataset object and map filenames to (image, label) tuples\n","    o_test_list_ds = tf.data.Dataset.from_tensor_slices(o_test_filenames)\n","    o_test_ds = im_tools.load_images_from_filenames(o_test_list_ds)"],"metadata":{"id":"S3o-hf08jYTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print distribution of loaded dataset\n","if POISONING_NAME != 'original':\n","    o_test_img_count = tf.data.experimental.cardinality(o_test_list_ds).numpy()\n","\n","    print(\"Total image count in testing set        : {}\".format(o_test_img_count))\n","    print(\"Normal images count in testing set      : {}\".format(get_class_count(\"Normal\", o_test_filenames)))\n","    print(\"Pneumonia images count in testing set   : {}\".format(get_class_count(\"Pneumonia\", o_test_filenames)))"],"metadata":{"id":"Zu1xNIpJjdOk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run the model trained on the poisoned set on the original test set\n","# and export predictions\n","if POISONING_NAME != 'original':\n","    o_test_ds_batch = o_test_ds.batch(BATCH_SIZE)\n","    o_predictions_file = OUTPUT_FOLDER + 'export_original_predictions.txt'\n","\n","    model_tools.predict_and_save_model_predictions(model, o_test_ds_batch, o_test_filenames, o_predictions_file)"],"metadata":{"id":"e2TrpA9hj2wA"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"4b7477865c3b10d64ced3258d391994d31c5d5216fb1f6f71b5cb53c89252681"}},"colab":{"provenance":[]},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}